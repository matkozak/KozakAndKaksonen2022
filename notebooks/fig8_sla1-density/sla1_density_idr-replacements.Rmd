---
title: "Sla1 patch density in Ede1 internal replacement mutants"
date: "Last compiled on `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE,
                      dpi = 96, fig.width = 6, fig.height = 4)
```

```{r libs}
# tidy stuff
library(tidyverse) 
library(broom) 
library(knitr)

# ggplot2 + extras
library(ggbeeswarm)
library(ggsignif)

# Extra tests and pairwise comparisons
library(multcompView)

# Mixed models and estimated marginal means
library(lmerTest)
library(emmeans)
```

```{r load}
# Load data generated by cleanup notebook
rm(list = ls())
load('data/idr_replacement_sla1.RData')
```

```{r theme}
# Custom ggplot2 theme
# --------------------

# minimal theme with border
# based on theme_linedraw without the grid lines
# also trying to remove all backgrounds and margins
# the aim is to make it as easy as possible to edit in illustrator

theme_clean <- function(base_size = 11, base_family = "",
                        base_line_size = base_size / 22,
                        base_rect_size = base_size / 22) {
  theme_linedraw(
    base_size = base_size,
    base_family = base_family,
    base_line_size = base_line_size,
    base_rect_size = base_rect_size
  ) %+replace%
    theme(
      # no grid and no backgrounds if I can help it
      legend.background =  element_blank(),
      panel.background = element_blank(),
      panel.grid = element_blank(),
      plot.background = element_blank(),
      plot.margin = margin(0, 0, 0, 0),
      complete = TRUE
    )
}

# Set default theme
# -----------------
theme_set(theme_clean(base_size = 14, base_family = "Myriad Pro"))

# Create a ggsave wrapper
# -----------------------

# This way we can set a default size and device for all plots
my_ggsave <- function(filename, plot = last_plot(),
                      device = cairo_pdf, units = "mm",
                      width = 100, height = 80, ...){
  ggsave(filename = filename, plot = plot,
         device = device, units = units,
         height = height, width = width,  ...)
  }
```

```{r functions}
# couple of small functions for extracting comparisons
# into a format understandable by ggplot

#' Add letter group labels generated by Tukey's HSD
#'
#' This function performs ANOVA and Tukey's HSD,
#' extracts the letter labels and attaches them
#' to the original data.
#'
#' @param x: df or tibble, the data (long format!)
#' @param yvar: chr, name of the dependent variable
#' @param xvar: chr, name of the independent variable
#' @param alpha: dbl, confidence level passed on to Tukey's test
#'
add_tukey_labels <- function(x, yvar, xvar, alpha = 0.95){
  aov_form <- formula(paste(yvar, '~', xvar))
  anova <- aov(formula = aov_form, data = x)
  tukey <- TukeyHSD(anova, which = xvar, conf.level = alpha)
  # Extract labels and factor levels from Tukey post-hoc 
  x_labels <- as_tibble(
    multcompLetters4(anova, tukey)[[xvar]]$Letters,
    rownames = xvar
    )
  
  if (is.factor(x[[xvar]])) {
    x_labels[[xvar]] <- factor(
      x_labels[[xvar]], levels = levels(x[[xvar]])
    )
  }
  
  x_labels <- rename(x_labels, tukey_group = value)
  x <- left_join(x, x_labels, by = xvar)
  
  return(x)
  }


#' Extract comparisons from rstatix tidy tests
#' 
#' This function subsets the selected comparisons in a Tukey,
#' Dunn or similar test done by rstatix.
#' It converts groups to a list of vectors that can be passed to geom_signif
#'
#'
#' @param x: df or tibble, the comparison results
#' @param rows: integer vector, the rows with desired comparisons
extract_comparisons <- function(x, rows){
  x_subset <- x[rows,] %>%
    .[nrow(.):1,]
  
  x_comparisons <- x_subset %>%
    select(group1, group2) %>%
    t() %>%
    as.data.frame() %>%
    as.list
  x_annotations <- x_subset$p.adj.signif %>%
    as.vector()
  
  significance <- list(
    comparisons = x_comparisons,
    annotations = x_annotations
  )
  
  return(significance)
}
```

# {.tabset .tabset-pills}

## Experiment

The aim of the experiment is to determine the density of Sla1-EGFP patches
in yeast mutants where the central region of Ede1 protein
(amino acids 366-900, PQ-rich and coiled-coil domains) 
was replaced by a heterologous protein domain, one of:
 
 - yeast Snf5 prion-like IDR
 - yeast Sup35 prion-like IDR
 - mCherry (monomeric fluorescent protein)
 - dTomato (dimeric fluorescent protein)
 - kinesin-1 coiled-coil from *Drosophila melanogaster* (dimeric)
 - kinesin-5 coiled-coil from *Homo sapiens* (tetrameric)

### Imaging

I acquired all data on the Olympus IX81
equipped with a 100x/1.49 objective,
using the X-Cite 120PC lamp at 50% intensity
and 400 ms exposure for illumination.
Light was filtered through a U-MGFPHQ filter cube.
I acquired stacks of 26 planes 
with a step size of 0.2 microns.

### Image processing

Individual non-budding cells were cropped from fields of view.
Patch numbers were extracted using Python function `count_patches`
from my personal package `mkimage`
containing a set of wrappers for `scikit-image` functions.
Briefly, the images were median-filtered with a 5 px disk brush,
and the filtered images were subtracted from the originals
to subtract local background.
The background-subtracted images were thresholded using the Yen method.
The thresholded images were eroded using the number of non-zero
neighbouring pixels in 3D as the erosion criterion.
The spots were counted using skimage.measure.label() function with 2-connectivity.

Cross-section area was obtained by 
median-filtering of the stack with 10px disk brush, 
calculating the maximum projection image, 
thresholding using Otsu's algorithm,
and using skimage.measure.regionprops() to measure area.
Note that these are pixel counts of *cross-section* area. 
To determine the total surface area, 
I assumed that an unbudded cell is spherical
(suface area is four times the cross-section area).

This call to `site_counter` was used to process all datasets:

```
process_folder(path, median_radius = 5, erosion_n = 1, con = 2,
                   method = Yen, mask = False, loop = False, save_images = True)
```

Another notebook was used to gather all output into tidy data frames
available here, with no further modifications.

### Strain list

```{r}
kable(strains)
```

## Per-dataset summary {.tabset}

### Patch number and area

```{r nums_dataset}
# means and SDs of number of patches and area
sla1_density %>%
  group_by(ede1, dataset) %>%
  summarise(n = n(),
            across(c(patches, area), list(mean = mean, sd = sd,
                                          se = ~sd(.x) / sqrt(n())))) %>%
  kable()
```

### Sla1 density

We can combine the patch number and area into
$density = \frac{patches}{area}$,
calculated individually for each cell.
We can summarise the data 
for each Ede1 mutant in each dataset:

```{r density}
sla1_density_stats <- sla1_density %>%
  group_by(ede1, dataset) %>%
  summarise(n = n(),
            across(density,
                   list(mean = mean, sd = sd, 
                        se = ~ sd(.x) / sqrt(n),
                        median = median, mad = mad)),
            .groups = 'drop')

sla1_density_stats %>% kable()
```

## Plots {.tabset}

```{r density.scatter}
plot_blank <- ggplot(sla1_density_stats,
                     aes(x = ede1, y = density_mean)) +
                         #shape = dataset, fill = dataset))+
  labs(title = NULL, x = 'Ede1', y = expression( "Sla1 patches/Âµm"^2)) +
  scale_y_continuous(breaks = seq(0.0, 1.0, 0.1)) +
  scale_shape_manual(values = c(21:25, 8)) +
  scale_color_brewer(palette = 'Set2') +
  scale_fill_brewer(palette = 'Set2')

plot_scatter <- plot_blank +
  geom_quasirandom(inherit.aes = F, data = sla1_density,
                   aes(x = ede1, y = density,
                       shape = dataset,# colour = dataset
                       ),
                   colour = 'grey75',# shape = 1,
                   show.legend = F, size = 0.8
                   )

plot_violin <- plot_blank + 
  geom_violin(inherit.aes = F,
              data = sla1_density, aes(x = ede1, y = density),
              colour = 'grey75', fill = 'transparent'
              )
```

### SuperPlots

I have chosen to show this data using the 
[SuperPlot](https://doi.org/10.1083/jcb.202001064) style.
Each point shows density of Sla1-EGFP patches in an individual cell.

Big colour points show mean measurements from three independent repeats.

Range is mean +/- SD, calculated based on the three independent repeat means.

#### Beeswarm

```{r}
plot_super <- plot_scatter +
  geom_quasirandom(aes(shape = dataset, fill = dataset),
                   show.legend = F,
                   width = 0.3, size = 2)+
  stat_summary(fun = mean, geom = 'crossbar',
               width = 0.5, fatten = 1)+
  stat_summary(fun.data = 'mean_sdl',
               fun.args = list(mult = 1), 
               geom = 'errorbar', width = 0.2)+
  guides(x = guide_axis(angle = -45))
  
print(plot_super)
my_ggsave('figures/density_super.pdf')

plot_super_flip <- plot_super +
  coord_flip()+
  scale_x_discrete(limits = rev(levels(sla1_density$ede1)))+
  theme(panel.border = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.line.x = element_line())

my_ggsave('figures/density_super_flip.pdf', plot_super_flip,
          height = 100, width = 80)
```

#### Violin

```{r}
plot_super_violin <- plot_violin +
  geom_quasirandom(aes(shape = dataset, fill = dataset),
                   show.legend = F,
                   width = 0.3, size = 2)+
  stat_summary(fun = mean, geom = 'crossbar',
               width = 0.5, fatten = 1)+
  stat_summary(fun.data = 'mean_sdl',
               fun.args = list(mult = 1),
               geom = 'errorbar', width = 0.2)

print(plot_super_violin)
my_ggsave('figures/density_super_violin.pdf')
```

### With significance

There are 9 levels of Ede1,
resulting in 36 possible pairwise comparisons.
This is too unwieldy for any plot,
at least a plot intended to also show the data.

As such, I propose to not even attempt
this kind of illustration.
An alternative is a compact letter display.
In this view, groups sharing at least one letter 
are not significantly different
at a chosen $\alpha$ (here, 5%).

Pros:

  * less cluttered and simpler to read
  * focus away from p-values; provide a binary decision on the null hypothesis
  
Cons:

  * cannot distinguish different confidence levels
  * can get complicated just as well
  * focuses on non-findings rather than findings.

```{r}
sla1_density_stats <- sla1_density_stats %>%
  add_tukey_labels('density_mean', 'ede1')
```

```{r}
plot_super_letters <- plot_super +
  geom_text(data = sla1_density_stats,
            aes(label = tukey_group), y = Inf, vjust = 1.2)
  #stat_summary(aes(label = tukey_group),
  #             fun.y = Inf,
  #             geom = 'text', na.rm = T, vjust = -0.5,
  #             #fun.args = list(err = errors, mult = error_range)
  #             )
print(plot_super_letters)
my_ggsave('figures/density_super_letters.pdf', width = 150)
```

As this is also complicated,
I propose a final solution:
indicate that all groups are significantly different
from wild-type in the text,
and mark those with a significant improvement from 
ede1âPQCC with a star,
all while providing a table of p-values
for each pairwise comparison in the supplement.

### Pooled box plot

This might be useful in situations
where this much data is too much data.

```{r}
plot_box <- plot_blank + 
  geom_boxplot(inherit.aes = F, data = sla1_density,
               aes(x = ede1, y = density),
               outlier.shape = 21, width = 0.5, notch = T)+
  guides(x = guide_axis(angle = -45))

print(plot_box)
```

## Modeling

We want to consider:

  1. What are the Sla1 densities in different Ede1 backgrounds?
  2. Are the differences between strains statistically significant?

The design is not ideal here:

  - we want to compare all mutants against each other
  - we want to block for nuisance factors,
  which might come from co-culturing the cells
  or things related to the microscope such as lamp power
  hence the independently repeated experiments
  - half the experiment
  was added *after* the reviewers request,
  so the design is disconnected. 
  Datasets 1-3 have both controls and IDRs,
  datasets 4-6 have the globular / coiled-coil replacements.
  Finally, the mCherry strain was added 
  after all of the others on separate dates.
  
Bad design cannot be solved
in statistical analysis,
but on the bright side, the data is nice
and there is no reason for a systematic error to be
associated with any particular dataset.
Since we are measuring an absolute property (number of sites),
and not something directly proportional
to exposure settings like intensity,
the influence of lamp power etc. should be limited

If the experiment was not disconnected,
we could try to see if the effects of date / dataset
can be modeled.

Because it *is* disconnected, it's a bit more tricky.
For example we cannot really get useful information
from trying to model `date` as a fixed effect after `ede1`.
What we could do for starters is to make a model
which disregards the dataset information, 
and look if the residuals are clustered by date or dataset,
informing us about whether this actually matters.

```{r echo = TRUE}
pooled_lm <- lm(density ~ ede1, data = sla1_density)
plot(as.factor(sla1_density$date), resid(pooled_lm),
     xlab = '', ylab = 'Residuals', las = 2,
     main = 'Univariate model residuals by date')
```

These seem to be very well distributed around zero.
The exception is the single mCherry acquisition
on 10/02/2022, which, to be honest,
does seem like an outlier.

So what to do? The decision seems fairly arbitrary.
I do not necessarily think that the univariate
model on all cells would be *wrong*,
but it does seem like a controversial issue.
A model of replicate means (*means-pooled analysis*)
will be conservative and lose power,
but it seems appropriate for once, 
considering the segregated design.

### Means-pooled linear model ANOVA

We will test the null hypothesis 
that mean Sla1 density is the same
across different Ede1 strains.
We will use repeat-level data for the tests
to account for experimental variability.

We can use ANOVA and a post-hoc test 
to find out how likely this data is to occur
under the null hypothesis.

More importantly,
we will try to find the effect sizes
of different contrasts.

We start by making a model
of `density_mean ~ ede1`:

```{r}
replicate_lm <- lm(density_mean ~ ede1, data = sla1_density_stats)
summary(replicate_lm)
anova(replicate_lm)
```

One-way ANOVA on means-pooled data 
rejects the null with $p = 1.5 \times 10^{-8}$.

#### Diagnostic plots

```{r}
plot(replicate_lm)
```

This is one problem with the small N of replicate means:
there are definitely differences in variance
for different levels, but we cannot tell
whether they reflect a difference in populations
or just noise. But it's probably the lattter;
if we go back to the cell-level observations
modeled by `pooled_lm` we can see that the residuals
are fine there, too:

```{r}
plot(pooled_lm)
```

### Estimated group means

The following are the model estimates 
of mean densities associates with each Ede1 level:

```{r}
replicate_emm <- emmeans(replicate_lm, spec = 'ede1')
summary(replicate_emm)
```

### Post-hoc test (Tukey)

We are not necessarily interested in every possible comparison,
but we do want to obtain all the p-values 
at least against âPQCC (is there a rescue?) and wild-type (is it complete?).
Tukey's test for contrasts of all 9 estimates:

```{r}
contrast(replicate_emm, method = 'pairwise')
```

This is a lot; perhaps we can put it in a graphical matrix.

#### p-value matrix

```{r fig.width=8}
p_matrix <- tidy(contrast(replicate_emm, method = 'pairwise'))%>%
  separate(contrast, c('gr1', 'gr2'), ' - ') %>%
  select(gr1, gr2, adj.p.value) %>%
  mutate(gr1 = fct_infreq(gr1), gr2 = fct_infreq(gr2))

p_matrix_plot <- ggplot(p_matrix, aes(gr1, gr2))+
  geom_tile(aes(fill = adj.p.value < 0.05), show.legend = F)+
  geom_text(aes(label = if_else(adj.p.value < 0.001,
                                '<0.001',
                                as.character(round(adj.p.value, 3)))))+
  labs(x = NULL, y = NULL)+
  #theme_clean()+
  theme(panel.border = element_blank(),
        axis.ticks = element_blank(),
        legend.title = element_text(face = 'bold'))+
  scale_fill_brewer()
print(p_matrix_plot)
my_ggsave('figures/p_matrix_plot.pdf', width = 150)
```

#### The bottom line

* every mutant is significantly different from wild-type at $\alpha = 0.05$
* only Sup35 replacement is a significant improvement over âPQCC

### Effect sizes

Effect sizes are perhaps more important than the p-values.
Here I will calculate the classical Cohen's d,
defined as the mean differences over population standard deviation.
Despite using means-pooled model
to estimate contrast p-values and population means,
I am confident that it is more appropriate
to take the pooled observation-level SD
for calculating the effect size.

Therefore $\sigma$ will be a mean of SD for each Ede1 level,
weighed by $n-1$ 
(departing from `emmeans` default suggestion of residual SD).
I am not quite sure what the appropriate degrees of freedom are,
I used sample size minus Ede1 level count. 
In any case, it affects the 95% CI's of effect size estimate,
but not the estimate itself.

Below are effect sizes calculated for comparisons against wild-type
and against âPQCC.

```{r}
pooled_var <- with(sla1_density,
     sqrt(weighted.mean(tapply(density, ede1, var),
                        tapply(density, ede1, length) - 1)))

eff_size(replicate_emm, sigma = pooled_var,
         edf = length(sla1_density$density) - length(levels(sla1_density$ede1)),
         method = 'trt.vs.ctrl', ref = 1)

eff_size(replicate_emm, sigma = pooled_var,
         edf = length(sla1_density$density) - length(levels(sla1_density$ede1)),
         method = 'trt.vs.ctrl', ref = 2)
```


### Addendum: mixed model analysis

The p-values for different contrasts above
are a result of a quite conservative model,
which itself is a result of suboptimal design. 
In a pooled observation model, for example, we would find 
a couple more significant effects due to larger sample size:

```{r}
pwpm(emmeans(pooled_lm, 'ede1'), diffs = F)
```

Here we get low p-values for the differences between Snf5 and everything else,
as well as âPQCC and actually *less* dense Khc/ede1â.

This model would generate group means 
and effect sizes pretty much the same as the means-pooled model,
but drastically different p-values.
This serves to underscore that p-values 
should be treated with healthy skepticism,
and play a minor role in the interpretation of the results.

Some would say that this model is an example of *pseudoreplication*,
but this is a bit of a grey area.
Yeast cells from different datasets
grow in the same medium, incubator and so on,
but they are individual organisms.
Even if a 'date effect' exists beyond a simple sampling variation,
there is no reason to think that the effect is consistent
across mutants: after all, each mutant grows in a separate tube.
If we try to look at an interaction plot,
we cannot see a consistent date effect:

```{r}
ggplot(sla1_density, aes(x = as_factor(date), y = density, group = ede1, col = ede1)) + 
  #scale_color_brewer(palette = "Set2")+
  stat_summary(fun = mean, geom = "line")+
  stat_summary(fun = mean, geom = "point")+
  guides(x = guide_axis(angle = -45))+
  labs(x = 'Date', y = 'Sla1 density')
```

Of course this is undermined
by the disconnect in date / mutant combinations.

Can we generate a linear mixed model for this,
using `date` as a random effect? 
Because there does not seem to be any consistent effect of date in itself,
let's add an interaction term.

```{r echo = TRUE}
date_lmm1 <- lmer(density ~ ede1 + (1|date), data = sla1_density)
date_lmm2 <- lmer(density ~ ede1 + (1|ede1:date), data = sla1_density)
date_lmm3 <- lmer(density ~ ede1 + (1|date) + (1|ede1:date), data = sla1_density)
```

```{r}
anova(date_lmm1, date_lmm2, date_lmm3)
```

Using the `anova` table for mixed models from `lmerTest`
we actually find the interaction-only model `date_lmm2` has the lowest AIC.

Single-term deletions of effects from the main + interaction model
`date_lmm3` suggest that the interaction model is significant
and the main date effect is not.

```{r}
ranova(date_lmm3)
```

What does this mean? I am getting out of my depth here,
but I think that we simply cannot assign
a meaningful change of intercept based on `date`
because the effect is different for every Ede1 level (hence the interaction).
Again, this makes sense because while theoretically there could be some issues
affecting all strains on a given day (say, incubator failure),
this just did not happen and so it's not reflected in the data
beyond a simple sampling error and that is going to be unique to each sample.

Note that this is very unlike the dataset in Figure 3,
where any change in the microscope 
will affect intensity readings.

Let's look at what the interaction-only model
says about `ede1` effects on `density`.
First, how do model residuals look?
Pretty good, but it's not like the pooled
observation-only model had any problems.

```{r}
plot(date_lmm2)

par(mfrow = c(1, 2))

qqnorm(ranef(date_lmm2)$"ede1:date"[, 1], 
       main = "Random effects of interaction")
qqnorm(resid(date_lmm2), main = "Residuals")
```

Even if we accept the interaction-only model,
the random factor does not explain much of the variance anyway:

```{r}
summary(date_lmm2)
```

We have `r round(100 * 0.0006184 / (0.0006184 + 0.0108813))`% of variance 
explained by the `1|ede1:date` term in the model.

Let's quickly generate group means (diagonal)
and p-values for contrasts, with Tukey adjustment:

```{r}
pwpm(emmeans(date_lmm2, 'ede1'), diffs = F)
```

The values are strikingly similar
to what we saw with the means-pooled model.
Why? I suspect that these are actually equivalent
because the mixed model we ended up with
creates 27 unique groups with no shared information
by `ede1` / `date` combination
just as we have 27 `ede1` / `dataset` unique group means
in the `replicate_lm` model.

## Conclusions

### Actual conclusions

1. All mutations cause a significant reduction in patch density from wild type
2. While Ede1âPQCC is not *significantly* different from ede1â, 
  it has to be noted that the estimate is higher
  (~40% reduction instead of ~50%).
3. Snf5 and Sup35 substitutions have significantly higher Sla1 densities than ede1â;
  Sup35 is also significantly different from âPQCC.
4. All the other mutants are similar across the board.
5. Stepping away from p-values: IDRs rescue Sla1 density only somewhat, 
  the others not at all. That is the take-home message.

### Final estimates

Final estimates with lower / upper 95% confidence intervals,
and a common-sense comparison to wild type (in %). 
`half_ci` is half of the confidence interval,
for writing CI ranges in the format mean +/- error.

```{r}
wt_mean <- sla1_density_stats %>%
  filter(ede1 == 'EDE1') %>%
  pull(density_mean) %>%
  mean()

density_ci <- sla1_density_stats %>%
  group_by(ede1)%>%
  summarise(mean_cl_normal(density_mean)) %>%
  rename(mean = y, lower = ymin, upper = ymax) %>%
  mutate(proc_wt = round(100 * mean / wt_mean),
         half_ci = (upper - lower) / 2) 

kable(density_ci, digits = 3)
```

## Source data

### .csv

```{r echo=FALSE}
xfun::embed_file('data/idr_replacement_sla1.csv')
```

### .RData

```{r echo=FALSE}
xfun::embed_file('data/idr_replacement_sla1.RData')
```

## Session info

```{r session, message=TRUE}
sessionInfo()
```

## WIP

```{r}
with(sla1_density, interaction.plot(x.factor = as.factor(date), trace.factor = ede1, 
                               response = density, las = 2, type = 'b'))
```
